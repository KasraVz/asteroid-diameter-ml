{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c4e3a5",
   "metadata": {},
   "source": [
    "# Thesis Documentation for `03_baselines.ipynb`\n",
    "\n",
    "This document provides a detailed methodological justification for the steps in the 03_baselines.ipynb notebook. The purpose of this notebook is to establish baseline performance metrics against which more complex models can be compared.\n",
    "\n",
    "## 3-A & 3-B: Workspace Initialization and Data Recreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "912bdb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-A: Imports & load essentials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "DATA      = Path(\"../data/asteroids_clean.csv\")\n",
    "PREPROC_P = Path(\"../data/preprocess.pkl\")\n",
    "\n",
    "df         = pd.read_csv(DATA)\n",
    "preprocess = joblib.load(PREPROC_P)   # fitted transformer from Step 2\n",
    "\n",
    "# 3-B: Recreate X and y\n",
    "TARGET = \"diameter\"\n",
    "DROP_ALWAYS = [\"Unnamed: 0\", \"GM\", \"G\", \"IR\", \"extent\",\n",
    "               \"UB\", \"BV\", \"spec_B\", \"spec_T\", \"name\",  # junk\n",
    "               \"per_y\"]                                 # duplicate of per\n",
    "\n",
    "X = df.drop(columns=[TARGET] + DROP_ALWAYS, errors=\"ignore\").copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "# Cast condition_code (0–9 quality label) to categorical\n",
    "X[\"condition_code\"] = X[\"condition_code\"].astype(\"object\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b4f3f1",
   "metadata": {},
   "source": [
    "# Thesis Justification\n",
    "## Objective:\n",
    "To create a consistent and reproducible environment for model training by loading all necessary data, objects, and libraries.\n",
    "\n",
    "## Methodology:\n",
    "The script imports the required libraries, loads the cleaned dataset (`asteroids_clean.csv`), and, crucially, loads the pre-fitted preprocessing pipeline (`preprocess.pkl`) that was saved in the previous notebook. The feature matrix X and target vector y are then recreated using the exact same logic as in the preprocessing notebook.\n",
    "\n",
    "## Justification:\n",
    "\n",
    "  - Consistency: By loading the saved `preprocess` object, we guarantee that the exact same imputation, scaling, and encoding parameters learned from the training set in notebook 02 are used here. This is fundamental to preventing data leakage and ensuring a valid comparison between models.\n",
    "\n",
    "  - Reproducibility: Re-executing the same feature selection and type casting steps ensures that the data fed into the pipeline has the precise structure the fitted pipeline expects. This makes the notebook a self-contained and verifiable unit of work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8069b578",
   "metadata": {},
   "source": [
    "**Purpose** Bring in scikit-learn, load the cleaned dataset, and load the\n",
    "already-fitted `preprocess.pkl` so every model sees the same transforms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9e5ca9",
   "metadata": {},
   "source": [
    "Mirror the exact column drops and type cast from Step 2 so the data\n",
    "arriving at `preprocess` has the layout it expects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a13dc7f",
   "metadata": {},
   "source": [
    "## 3-C: Consistent Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a81d1279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=RANDOM_STATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a8ec02",
   "metadata": {},
   "source": [
    "# Thesis Justification\n",
    "## Objective:\n",
    "To partition the data into training and validation sets that are identical to those used in the preprocessing step.\n",
    "\n",
    "## Methodology:\n",
    "`train_test_split` is called with the same `test_size` (0.20) and, most importantly, the same `random_state` (42) as used previously.\n",
    "\n",
    "## Justification:\n",
    "Using an identical `random_state` is non-negotiable for sound model comparison. It ensures that the `X_train` and `X_val` sets in this notebook contain the exact same data points as the sets used to fit and evaluate the preprocessor. This consistency is the only way to ensure that differences in performance are due to the models themselves, not variations in the data they are trained or evaluated on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7746b8",
   "metadata": {},
   "source": [
    "Hold out 20 % of rows as a validation set (identical random seed as\n",
    "before) so scores are comparable across all models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3ca9c6",
   "metadata": {},
   "source": [
    "# Baseline 1 – DummyRegressor (median)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b46376",
   "metadata": {},
   "source": [
    "## 3-D: Fit & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "391c97d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = Pipeline([\n",
    "    (\"prep\", preprocess),                     # already fitted\n",
    "    (\"reg\",  DummyRegressor(strategy=\"median\"))\n",
    "])\n",
    "\n",
    "dummy_model.fit(X_train, y_train)\n",
    "y_pred_dummy = dummy_model.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6612e79e",
   "metadata": {},
   "source": [
    "# Thesis Justification\n",
    "## Objective:\n",
    "To establish an absolute performance floor by creating a non-intelligent model. Any subsequent, more complex model must outperform this baseline to be considered useful.\n",
    "\n",
    "## Methodology:\n",
    "A `DummyRegressor` is placed into a `Pipeline` with the pre-fitted `preprocess` object. The `strategy=\"median\"` instructs the model to simply predict the median value of the training set's target (`y_train`) for every single instance in the validation set.\n",
    "\n",
    "## Justification:\n",
    "The `DummyRegressor` serves as a \"sanity check.\" It answers the question: \"How well can we predict the diameter if we ignore all the features and just guess the most typical value?\" The resulting performance metrics represent the lower bound of what is achievable. A negative R² score, for example, would indicate that a model is performing worse than this naïve strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fe8356",
   "metadata": {},
   "source": [
    "**Dummy (median)** simply predicts the training-set median for every\n",
    "asteroid.  This sets the absolute performance floor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125598a",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3062a348",
   "metadata": {},
   "source": [
    "## 3-E: Defining the Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1508514b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 2.655770833333333,\n",
       " 'RMSE': 6.86362135647065,\n",
       " 'R²': -0.04744953304303601}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# 1️⃣  Try to import the new function (exists in sklearn ≥ 1.4)\n",
    "try:\n",
    "    from sklearn.metrics import root_mean_squared_error  # noqa\n",
    "    def _rmse(y_true, y_pred):\n",
    "        return root_mean_squared_error(y_true, y_pred)\n",
    "except ImportError:\n",
    "    # 2️⃣  Fallback for older sklearn: manual sqrt of mean_squared_error\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    def _rmse(y_true, y_pred):\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"MAE\":  mean_absolute_error(y_true, y_pred),\n",
    "        \"RMSE\": _rmse(y_true, y_pred),\n",
    "        \"R²\":   r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "scores_dummy = metrics(y_val, y_pred_dummy)\n",
    "scores_dummy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee30af59",
   "metadata": {},
   "source": [
    "# Thesis Justification\n",
    "## Objective:\n",
    "To define a standardized set of metrics for evaluating and comparing all regression models in this project.\n",
    "\n",
    "## Methodology:\n",
    "A helper function `metrics` is created to compute three standard regression metrics: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R-squared (R²). A compatibility wrapper `_rmse` is included to handle different versions of scikit-learn.\n",
    "\n",
    "## Justification of Metric Choices:\n",
    "\n",
    " - Mean Absolute Error (MAE): Measures the average absolute difference between the predicted and actual values. It is easily interpretable as the typical prediction error in the original units of the target (km).\n",
    "\n",
    " - Root Mean Squared Error (RMSE): This metric squares the errors before averaging, which penalizes larger errors more heavily than smaller ones. It is also in the original units of the target, making it interpretable.\n",
    "\n",
    " - R-squared (R²): The coefficient of determination represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). An R² of 1 indicates perfect prediction, while an R² of 0 indicates the model performs no better than predicting the mean. Negative values indicate the model is worse than the mean-predicting baseline.\n",
    "\n",
    "## Code Robustness:\n",
    "The `try-except` block for RMSE calculation is a best practice for writing shareable, long-lasting research code. It ensures the notebook will run correctly in different environments, whether they have the latest scikit-learn version (which includes `root_mean_squared_error`) or an older one (requiring the manual `np.sqrt(mean_squared_error(...))`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69f721",
   "metadata": {},
   "source": [
    "We record **MAE**, **RMSE** and **R²** for the naïve “predict‐the-median” model.  \n",
    "These values are the absolute floor: every real model must beat them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbd9819",
   "metadata": {},
   "source": [
    "# Baseline 2: LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604dfcc6",
   "metadata": {},
   "source": [
    "## 3-F & 3-G: Fit, Predict, and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "407c620e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 2.226734298170547, 'RMSE': 9.30398774148489, 'R²': -0.9247074738179806}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-F: Fit & predict\n",
    "lin_model = Pipeline([\n",
    "    (\"prep\", preprocess),          # reuse the fitted pre-processor\n",
    "    (\"reg\",  LinearRegression())\n",
    "])\n",
    "\n",
    "lin_model.fit(X_train, y_train)\n",
    "y_pred_lin = lin_model.predict(X_val)\n",
    "\n",
    "# 3-G: Evaluate LinearRegression\n",
    "scores_lin = metrics(y_val, y_pred_lin)\n",
    "scores_lin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae901d",
   "metadata": {},
   "source": [
    "# Thesis Justification\n",
    "## Objective:\n",
    "To establish the performance of a simple, classical linear model. This serves as the first \"intelligent\" baseline.\n",
    "\n",
    "## Methodology:\n",
    "A `LinearRegression` model is placed in a `Pipeline` and trained on the preprocessed training data. Predictions are made on the validation set, and the same `metrics` function is used to evaluate its performance.\n",
    "\n",
    "## Justification:\n",
    "Linear Regression is an excellent baseline because it is fast, interpretable, and provides a clear signal of whether linear relationships exist between the engineered features and the target. If this simple model shows a significant improvement over the `DummyRegressor` (e.g., lower MAE/RMSE, higher R²), it validates that the feature engineering process has successfully extracted predictive information. It sets a new, more challenging performance target for more complex, non-linear models (like tree ensembles) to beat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8b1a27",
   "metadata": {},
   "source": [
    "A straight-line model in the engineered feature space.  \n",
    "Runs instantly and shows whether simple linear relationships capture real signal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fff27a",
   "metadata": {},
   "source": [
    "If MAE and RMSE drop (and R² rises) compared with the Dummy baseline,\n",
    "we’ve proven even a basic linear model learns something meaningful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9ccb53",
   "metadata": {},
   "source": [
    "## 3H ― Side-by-side comparison table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "800d4e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dummy (median)</th>\n",
       "      <td>2.655771</td>\n",
       "      <td>6.863621</td>\n",
       "      <td>-0.047450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>2.226734</td>\n",
       "      <td>9.303988</td>\n",
       "      <td>-0.924707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       MAE      RMSE        R²\n",
       "Dummy (median)    2.655771  6.863621 -0.047450\n",
       "LinearRegression  2.226734  9.303988 -0.924707"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-H — put both rows in one DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(\n",
    "    [scores_dummy, scores_lin],\n",
    "    index=[\"Dummy (median)\", \"LinearRegression\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df431b6f",
   "metadata": {},
   "source": [
    "# Thesis Justification\n",
    "## Objective:\n",
    "To present the performance metrics of the baseline models in a clear, concise, and easily comparable format.\n",
    "\n",
    "## Methodology:\n",
    "The dictionaries containing the scores for each model are used to construct a pandas DataFrame.\n",
    "\n",
    "## Justification:\n",
    "A summary table is the standard and most effective way to compare model performance in a research context. It allows for at-a-glance assessment of the relative strengths and weaknesses of each approach. This table will serve as the foundation for model selection, with the results from more advanced models being added as new rows in subsequent notebooks. This systematic comparison is essential for justifying the final choice of model for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a0be65",
   "metadata": {},
   "source": [
    "A single table makes it clear how much LinearRegression improves over the\n",
    "naïve baseline.  \n",
    "Future models (Random Forest, Gradient Boosting, …) will be added as extra rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efbaf3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: could not open directory 'notebooks/notebooks/': No such file or directory\n",
      "fatal: pathspec 'notebooks/03_baselines.ipynb' did not match any files\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   02_preprocessing.ipynb\u001b[m\n",
      "\t\u001b[31mmodified:   03_baselines.ipynb\u001b[m\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31m../data/data.csv\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "!git add notebooks/03_baselines.ipynb\n",
    "!git commit -m \"Step 3: evaluated Dummy & Linear baselines\"\n",
    "!git push\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asteroid-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
