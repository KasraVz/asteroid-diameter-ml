{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23255f1c",
   "metadata": {},
   "source": [
    "This notebook takes the cleaned data from the initial exploratory analysis and prepares it for machine learning by performing feature engineering, selection, and scaling.\n",
    "\n",
    "# Project Initialization\n",
    "## Objective\n",
    "To set up the environment by loading the pre-cleaned dataset and the necessary libraries for data processing and modeling.\n",
    "\n",
    "## Methodology\n",
    "We will load the `asteroids_clean.csv` file, which was the output of the previous notebook (`01_eda.ipynb`). We also import specific modules from `sklearn` for model selection and preprocessing, and `joblib` for saving Python objects.\n",
    "\n",
    "## Justification & Alternatives\n",
    "- Loading Clean Data: Starting from the cleaned CSV ensures that we are building upon a consistent and verified baseline, making the workflow more modular and efficient.\n",
    "\n",
    "- Library Choices:\n",
    "\n",
    "  - sklearn.model_selection: This module contains `train_test_split`, the standard function for partitioning data.\n",
    "\n",
    "  - sklearn.preprocessing: This module contains `StandardScaler`, a common tool for feature scaling.\n",
    "\n",
    "  - joblib: This library is preferred for saving and loading Python objects, especially those containing large NumPy arrays (like scikit-learn models and scalers), as it is more efficient than alternatives like `pickle`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36559284",
   "metadata": {},
   "source": [
    "## 2-B: Import Libraries & Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0827afce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T21:30:07.303728Z",
     "iopub.status.busy": "2025-07-07T21:30:07.303275Z",
     "iopub.status.idle": "2025-07-07T21:30:07.760066Z",
     "shell.execute_reply": "2025-07-07T21:30:07.759545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1436, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>a</th>\n",
       "      <th>e</th>\n",
       "      <th>i</th>\n",
       "      <th>om</th>\n",
       "      <th>w</th>\n",
       "      <th>q</th>\n",
       "      <th>ad</th>\n",
       "      <th>per_y</th>\n",
       "      <th>data_arc</th>\n",
       "      <th>...</th>\n",
       "      <th>UB</th>\n",
       "      <th>IR</th>\n",
       "      <th>spec_B</th>\n",
       "      <th>spec_T</th>\n",
       "      <th>G</th>\n",
       "      <th>moid</th>\n",
       "      <th>class</th>\n",
       "      <th>n</th>\n",
       "      <th>per</th>\n",
       "      <th>ma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.038918</td>\n",
       "      <td>0.069094</td>\n",
       "      <td>9.948162</td>\n",
       "      <td>217.408407</td>\n",
       "      <td>95.637570</td>\n",
       "      <td>2.828947</td>\n",
       "      <td>3.248890</td>\n",
       "      <td>5.297692</td>\n",
       "      <td>10333.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.83752</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.186048</td>\n",
       "      <td>1934.982100</td>\n",
       "      <td>226.241935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.781803</td>\n",
       "      <td>0.200606</td>\n",
       "      <td>9.233482</td>\n",
       "      <td>19.677473</td>\n",
       "      <td>164.054480</td>\n",
       "      <td>2.223758</td>\n",
       "      <td>3.339848</td>\n",
       "      <td>4.639784</td>\n",
       "      <td>7498.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.22752</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.212429</td>\n",
       "      <td>1694.681031</td>\n",
       "      <td>97.864386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.532657</td>\n",
       "      <td>0.150951</td>\n",
       "      <td>7.307953</td>\n",
       "      <td>152.847672</td>\n",
       "      <td>256.627796</td>\n",
       "      <td>2.150350</td>\n",
       "      <td>2.914963</td>\n",
       "      <td>4.030627</td>\n",
       "      <td>10256.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.17367</td>\n",
       "      <td>MBA</td>\n",
       "      <td>0.244534</td>\n",
       "      <td>1472.186639</td>\n",
       "      <td>135.680806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  name         a         e         i          om           w         q  \\\n",
       "0  NaN  3.038918  0.069094  9.948162  217.408407   95.637570  2.828947   \n",
       "1  NaN  2.781803  0.200606  9.233482   19.677473  164.054480  2.223758   \n",
       "2  NaN  2.532657  0.150951  7.307953  152.847672  256.627796  2.150350   \n",
       "\n",
       "         ad     per_y  data_arc  ...  UB  IR  spec_B spec_T   G     moid  \\\n",
       "0  3.248890  5.297692   10333.0  ... NaN NaN     NaN    NaN NaN  1.83752   \n",
       "1  3.339848  4.639784    7498.0  ... NaN NaN     NaN    NaN NaN  1.22752   \n",
       "2  2.914963  4.030627   10256.0  ... NaN NaN     NaN    NaN NaN  1.17367   \n",
       "\n",
       "   class         n          per          ma  \n",
       "0    MBA  0.186048  1934.982100  226.241935  \n",
       "1    MBA  0.212429  1694.681031   97.864386  \n",
       "2    MBA  0.244534  1472.186639  135.680806  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "DATA = Path(\"../data/asteroids_clean.csv\")   # file you saved in Step 1\n",
    "df = pd.read_csv(DATA)\n",
    "\n",
    "print(df.shape)     # expect (1436, 31)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457a9140",
   "metadata": {},
   "source": [
    "# Thesis Justification\n",
    "## Objective: \n",
    "To initialize the workspace for the preprocessing stage.\n",
    "\n",
    "## Methodology:\n",
    "The necessary libraries (`pandas`, `numpy`, `pathlib`) are imported. The `asteroids_clean.csv` file, which is the verified output from the Exploratory Data Analysis (EDA) phase (notebook `01_eda.ipynb`), is loaded into a pandas DataFrame. A global random seed is set to ensure reproducibility.\n",
    "\n",
    "## Justification:\n",
    "\n",
    " - Reproducibility: Setting a `RANDOM_STATE` is a cornerstone of scientifically valid computational research. It ensures that any stochastic process, such as the train-test split performed later, is deterministic and can be precisely replicated by others.\n",
    "\n",
    " - Modularity: By loading the pre-cleaned CSV, this notebook builds upon the validated work of the previous step. This modular approach makes the project workflow more organized, efficient, and less prone to errors, as the initial data cleaning does not need to be repeated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b4148",
   "metadata": {},
   "source": [
    "**What we do**  \n",
    "1. Import pandas/NumPy.  \n",
    "2. Set a global random seed for reproducibility.  \n",
    "3. Load the cleaned CSV produced in Step 1.  \n",
    "\n",
    "**Why**  \n",
    "Everyone on the team starts from the same dataset and gets identical\n",
    "train/validation splits when we use `random_state=42`.\n",
    "\n",
    "*Expected output* → **(1436, 31)** rows × columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b111b233",
   "metadata": {},
   "source": [
    "## 2-C: Define Target, Drop Uninformative Columns, and Group Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef4a93",
   "metadata": {},
   "source": [
    "### Code Block 1: Feature/Target Separation and Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51f63cec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T21:30:07.762999Z",
     "iopub.status.busy": "2025-07-07T21:30:07.762783Z",
     "iopub.status.idle": "2025-07-07T21:30:07.767851Z",
     "shell.execute_reply": "2025-07-07T21:30:07.767467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1436, 21)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET = \"diameter\"\n",
    "\n",
    "DROP_ALWAYS = [\n",
    "    \"Unnamed: 0\",                 # ghost index (may already be absent)\n",
    "    \"GM\", \"G\", \"IR\", \"extent\",    # 100 % missing\n",
    "    \"UB\", \"BV\", \"spec_B\", \"spec_T\",  # > 99 % missing\n",
    "    \"name\"                        # mostly NaN and an arbitrary ID\n",
    "]\n",
    "\n",
    "X = df.drop(columns=[TARGET] + DROP_ALWAYS, errors=\"ignore\")\n",
    "y = df[TARGET]\n",
    "\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2879c4",
   "metadata": {},
   "source": [
    "# Thesis Justification\n",
    "## Objective:\n",
    "To perform initial feature selection by removing columns that provide no predictive value and to formally separate the predictor features (`X`) from the target variable (`y`).\n",
    "\n",
    "## Methodology:\n",
    "The target variable `diameter` is defined. A list, `DROP_ALWAYS`, contains columns identified during EDA as uninformative. These columns, along with the target, are dropped from the original DataFrame to create the feature matrix `X`. The target variable `y` is created as a separate Series.\n",
    "\n",
    "## Justification of Dropped Columns:\n",
    "\n",
    " - No Information Content: `GM`, `G`, `IR`, `extent` are removed as they were found to be 100% null. `UB`, `BV`, `spec_B`, `spec_T` are removed due to having over 99% missing values; imputing such a high percentage would introduce more noise than signal.\n",
    "\n",
    " - Identifier, Not a Feature: `name` is a unique identifier. Including it would risk data leakage, where a model could simply memorize the diameter for a given name rather than learning a generalizable physical relationship.\n",
    "\n",
    " - `errors=\"ignore\"`: This argument makes the code robust by preventing an error if a column in DROP_ALWAYS has already been removed or is not present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a097b0d",
   "metadata": {},
   "source": [
    "**What**  \n",
    "• Separate features `X` from the regression target `y`.  \n",
    "• Remove columns that cannot inform the model (all-missing or ID-like).\n",
    "\n",
    "**Why**  \n",
    "Dropping junk early keeps the pipeline lightweight and avoids leaking\n",
    "an identifier (`name`) that the model could memorise instead of learning\n",
    "real patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2223d62d",
   "metadata": {},
   "source": [
    "## Code Block 2: Data Type Correction and Redundancy Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d20781d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T21:30:07.769891Z",
     "iopub.status.busy": "2025-07-07T21:30:07.769719Z",
     "iopub.status.idle": "2025-07-07T21:30:07.773256Z",
     "shell.execute_reply": "2025-07-07T21:30:07.772835Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cast condition_code (0–9 quality rating) to categorical\n",
    "X[\"condition_code\"] = X[\"condition_code\"].astype(\"object\")\n",
    "\n",
    "# per  = orbital period in days  |  per_y = same in years\n",
    "# Keep just one to avoid perfect collinearity\n",
    "X = X.drop(columns=[\"per_y\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f8f321",
   "metadata": {},
   "source": [
    "# Thesis Justification\n",
    "## Objective:\n",
    "To correct data types for categorical variables and remove redundant features to avoid multicollinearity.\n",
    "\n",
    "## Methodology:\n",
    "The `condition_code` column is explicitly cast to the `object` data type. The `per_y` (orbital period in years) column is dropped.\n",
    "\n",
    "## Justification:\n",
    "\n",
    " - `condition_code` as Categorical: Although represented by numbers, the `condition_code` is a nominal label indicating orbit quality. Treating it as a number would imply a false ordinal relationship (e.g., that code '9' is nine times '1'). Casting it to `object` ensures it will be correctly one-hot encoded as a categorical feature.\n",
    "\n",
    " - Multicollinearity: per (period in days) and per_y (period in years) are perfectly correlated as they measure the same physical quantity in different units. Including both would introduce perfect multicollinearity, which can destabilize the coefficient estimates of linear models. Removing one is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8323a856",
   "metadata": {},
   "source": [
    "**What**  \n",
    "1. `condition_code` is a *label* (0–9), not a quantity → treat it as a\n",
    "   category so the model gets one-hot dummies.  \n",
    "2. `per_y` duplicates `per`; we keep `per` (days) and drop the years\n",
    "   version.\n",
    "\n",
    "**Why**  \n",
    "Categorical coding prevents the model from interpreting “code 9” as\n",
    "nine times something.  Removing duplicate signals avoids redundant,\n",
    "perfectly correlated features that can mislead linear models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf0dbd3",
   "metadata": {},
   "source": [
    "## Code Block 3: Programmatic Column Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "491297bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T21:30:07.775193Z",
     "iopub.status.busy": "2025-07-07T21:30:07.775059Z",
     "iopub.status.idle": "2025-07-07T21:30:07.779800Z",
     "shell.execute_reply": "2025-07-07T21:30:07.779444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 numeric  |  4 categorical\n",
      "Categoricals: ['condition_code', 'neo', 'pha', 'class']\n"
     ]
    }
   ],
   "source": [
    "NUMERIC_COLS     = X.select_dtypes([\"int64\", \"float64\"]).columns.tolist()\n",
    "CATEGORICAL_COLS = X.select_dtypes([\"object\", \"bool\"]).columns.tolist()\n",
    "\n",
    "print(f\"{len(NUMERIC_COLS)} numeric  |  {len(CATEGORICAL_COLS)} categorical\")\n",
    "print(\"Categoricals:\", CATEGORICAL_COLS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66e4618",
   "metadata": {},
   "source": [
    "# Thesis Justification\n",
    "## Objective:\n",
    "To programmatically separate the column names into numeric and categorical groups.\n",
    "\n",
    "## Methodology:\n",
    "The `select_dtypes` method is used to automatically identify and list the names of columns belonging to numeric and categorical types.\n",
    "\n",
    "## Justification:\n",
    "This automated approach is more robust and less error-prone than manually defining these lists. These lists are critical inputs for the `ColumnTransformer` in the next step, ensuring that the correct preprocessing steps (e.g., scaling vs. one-hot encoding) are applied to the appropriate columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e16d15",
   "metadata": {},
   "source": [
    "**What**  \n",
    "Ask pandas for two column lists: numeric and categorical.\n",
    "\n",
    "**Why**  \n",
    "These lists feed the ColumnTransformer so each branch (scaling vs\n",
    "one-hot) knows exactly which columns to handle.\n",
    "\n",
    "*Expected* → **17 numeric | 4 categorical**  \n",
    "(`neo`, `pha`, `class`, `condition_code`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cc6ee6",
   "metadata": {},
   "source": [
    "## 2-D: Build the Column-wise Preprocessing Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4784f347",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T21:30:07.781812Z",
     "iopub.status.busy": "2025-07-07T21:30:07.781586Z",
     "iopub.status.idle": "2025-07-07T21:30:09.040612Z",
     "shell.execute_reply": "2025-07-07T21:30:09.040185Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "numeric_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\",  StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\", fill_value=\"Missing\")),\n",
    "    (\"encode\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", numeric_pipe, NUMERIC_COLS),\n",
    "    (\"cat\", categorical_pipe, CATEGORICAL_COLS)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3779253",
   "metadata": {},
   "source": [
    "# Thesis Justification\n",
    "## Objective:\n",
    "To construct a comprehensive preprocessing pipeline that handles missing values and applies appropriate transformations to numeric and categorical data types separately.\n",
    "\n",
    "## Methodology:\n",
    "A `ColumnTransformer` is defined, which applies two distinct sub-pipelines (`numeric_pipe` and `categorical_pipe`) to their respective column groups.\n",
    "\n",
    "## Justification of `numeric_pipe`:\n",
    "\n",
    " - Imputation Strategy: `SimpleImputer(strategy=\"median\")` is chosen to fill missing numerical values. The median is a robust measure of central tendency that is less sensitive to outliers than the mean, which is a desirable property for astronomical data that can have extreme values.\n",
    "\n",
    "- Scaling Strategy: `StandardScaler` is used to standardize features by removing the mean and scaling to unit variance. This is crucial for distance-based algorithms (e.g., SVMs) and gradient-based algorithms (e.g., linear regression), ensuring that all features contribute equally to the model's objective function regardless of their original scale.\n",
    "\n",
    "## Justification of `categorical_pipe`:\n",
    "\n",
    " - Imputation Strategy: `SimpleImputer(strategy=\"most_frequent\")` is a standard approach for filling missing categorical labels.\n",
    "\n",
    " - Encoding Strategy: `OneHotEncoder` is the correct method for converting nominal categorical data into a numerical format without implying an artificial order. The argument `handle_unknown=\"ignore\"` makes the pipeline robust to new, unseen categories during prediction, preventing errors if the test set contains a category not seen in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b672cc9",
   "metadata": {},
   "source": [
    "**What**  \n",
    "*Numeric branch*  \n",
    "  • Impute NaNs with the **median** (robust to outliers).  \n",
    "  • Standard-scale to mean 0 / std 1.\n",
    "\n",
    "*Categorical branch*  \n",
    "  • Replace NaNs with the **most-frequent** label (or “Missing”).  \n",
    "  • One-hot encode; `handle_unknown=\"ignore\"` keeps the model alive when\n",
    "    it sees a brand-new category later.\n",
    "\n",
    "**Why**  \n",
    "Encapsulating every step in a Pipeline guarantees the exact same\n",
    "transforms are applied during cross-validation and on the real test\n",
    "data — eliminating data-leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4173213",
   "metadata": {},
   "source": [
    "## 2-E Train / validation split before fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "097d42ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T21:30:09.043080Z",
     "iopub.status.busy": "2025-07-07T21:30:09.042868Z",
     "iopub.status.idle": "2025-07-07T21:30:09.047366Z",
     "shell.execute_reply": "2025-07-07T21:30:09.047035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1148, 20) (288, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5388ecc",
   "metadata": {},
   "source": [
    "# Thesis Justification\n",
    "## Objective:\n",
    "To partition the dataset into a training set for model development and a validation set for unbiased performance evaluation.\n",
    "\n",
    "## Methodology:\n",
    "The `train_test_split` function is used to reserve 20% of the data for validation (`X_val`, `y_val`).\n",
    "\n",
    "## Justification:\n",
    "This is the most critical step for preventing data leakage. The validation set simulates unseen data. By splitting the data before fitting the preprocessing pipeline, we ensure that the imputer and scaler learn their parameters (e.g., medians, means, standard deviations) only from the training data. This prevents any information from the validation set from \"leaking\" into the training process, which would lead to overly optimistic and invalid performance metrics. The 80/20 split is a standard convention that balances the need for sufficient training data with a robust validation set size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a7deb",
   "metadata": {},
   "source": [
    "**What**  \n",
    "Reserve 20 % of the data for **validation**.\n",
    "\n",
    "**Why**  \n",
    "We must assess model quality on unseen data.  Splitting *before*\n",
    "calling `preprocess.fit()` ensures the imputer and scaler learn only\n",
    "from the training subset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8fc8ad",
   "metadata": {},
   "source": [
    "## 2-F: Fit and Apply the Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2548826",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T21:30:09.049259Z",
     "iopub.status.busy": "2025-07-07T21:30:09.049099Z",
     "iopub.status.idle": "2025-07-07T21:30:09.069928Z",
     "shell.execute_reply": "2025-07-07T21:30:09.069572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix → (1148, 39)\n",
      "Validation  → (288, 39)\n"
     ]
    }
   ],
   "source": [
    "preprocess.fit(X_train)\n",
    "\n",
    "X_train_ready = preprocess.transform(X_train)\n",
    "X_val_ready   = preprocess.transform(X_val)\n",
    "\n",
    "print(\"Train matrix →\", X_train_ready.shape)\n",
    "print(\"Validation  →\", X_val_ready.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3096c3",
   "metadata": {},
   "source": [
    "# Thesis Justification\n",
    "## Objective:\n",
    "To apply the defined preprocessing steps to the training and validation sets.\n",
    "\n",
    "## Methodology:\n",
    "The `preprocess` pipeline is first fitted to the training data (`X_train`) using the `.fit()` method. Then, the fitted pipeline is used to transform both the training and validation sets using the `.transform()` method.\n",
    "\n",
    "## Justification:\n",
    "The \"fit on train, transform on both\" paradigm is strictly followed.\n",
    "\n",
    " - `preprocess.fit(X_train)`: This step learns the necessary parameters for transformation (medians for imputation, means/stds for scaling, vocabulary for one-hot encoding) exclusively from the training data.\n",
    "\n",
    " - `preprocess.transform(...)`: This step applies the learned transformations consistently to both datasets, ensuring that the validation data is processed in the exact same manner as the training data, which is essential for a fair evaluation. The final shapes are checked as a sanity test to confirm the pipeline executed correctly and the one-hot encoding expanded the feature space as expected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286a6f2",
   "metadata": {},
   "source": [
    "**What**  \n",
    "• `.fit()` learns medians, most-frequent labels, scaling parameters, and\n",
    "  one-hot vocabularies **only from the training data**.  \n",
    "• `.transform()` converts raw rows into a pure-numeric matrix.\n",
    "\n",
    "**Why**  \n",
    "Checking the dimensions confirms all columns (plus one-hot expansions)\n",
    "are present and identical in train & validation matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3050cdc4",
   "metadata": {},
   "source": [
    "## Code: Saving the Fitted Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db16f92b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T21:30:09.071891Z",
     "iopub.status.busy": "2025-07-07T21:30:09.071721Z",
     "iopub.status.idle": "2025-07-07T21:30:09.076647Z",
     "shell.execute_reply": "2025-07-07T21:30:09.076297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/preprocess.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib, pathlib\n",
    "joblib.dump(preprocess, pathlib.Path(\"../data/preprocess.pkl\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48346bf",
   "metadata": {},
   "source": [
    "# Thesis Justification\n",
    "## Objective:\n",
    "To persist the fitted preprocessing pipeline for future use.\n",
    "\n",
    "## Methodology:\n",
    "The `joblib.dump` function is used to serialize and save the entire fitted `preprocess` object to a file.\n",
    "\n",
    "## Justification:\n",
    "Saving the fitted pipeline is crucial for reproducibility and deployment. It allows subsequent notebooks (for modeling) or production scripts to load the exact same transformation and apply it to new data without having to retrain it. This guarantees that any new data is processed identically to the original training data, which is a requirement for making valid predictions. `joblib` is generally preferred over `pickle` for scikit-learn objects as it can be more efficient with objects containing large NumPy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e82b4f6",
   "metadata": {},
   "source": [
    "Saving the fitted transformer lets teammates (or a deployment script)\n",
    "load it instantly:\n",
    "\n",
    "```python\n",
    "preprocess = joblib.load(\"../data/preprocess.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd49d03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T21:28:43.642753Z",
     "iopub.status.busy": "2025-07-07T21:28:43.642599Z",
     "iopub.status.idle": "2025-07-07T21:28:53.412757Z",
     "shell.execute_reply": "2025-07-07T21:28:53.411517Z"
    }
   },
   "outputs": [],
   "source": [
    "!git add notebooks/02_preprocessing.ipynb data/preprocess.pkl\n",
    "!git commit -m \"Step 2: complete preprocessing pipeline\"\n",
    "!git push\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asteroid-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
