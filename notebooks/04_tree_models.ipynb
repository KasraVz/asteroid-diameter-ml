{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb2cf6b",
   "metadata": {},
   "source": [
    "# Step 4 – Tree ensembles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f33a2",
   "metadata": {},
   "source": [
    "## 4A ― Imports & data reload (code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53046a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 – Tree ensembles\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Same dataset & fitted preprocessor\n",
    "DATA      = Path(\"../data/asteroids_clean.csv\")\n",
    "PREPROC_P = Path(\"../data/preprocess.pkl\")\n",
    "\n",
    "df         = pd.read_csv(DATA)\n",
    "preprocess = joblib.load(PREPROC_P)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fbc084",
   "metadata": {},
   "source": [
    "Load the clean data and the *fitted* `preprocess` object so we can bolt\n",
    "tree models on top.  We import RandomForest & GradientBoosting plus\n",
    "cross-validation helpers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d508e6",
   "metadata": {},
   "source": [
    "## 4B ― Rebuild X/y and split once (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89772fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"diameter\"\n",
    "DROP_ALWAYS = [\"Unnamed: 0\", \"GM\", \"G\", \"IR\", \"extent\",\n",
    "               \"UB\", \"BV\", \"spec_B\", \"spec_T\", \"name\", \"per_y\"]\n",
    "\n",
    "X = df.drop(columns=[TARGET] + DROP_ALWAYS, errors=\"ignore\").copy()\n",
    "y = df[TARGET].copy()\n",
    "X[\"condition_code\"] = X[\"condition_code\"].astype(\"object\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=RANDOM_STATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940c7fc1",
   "metadata": {},
   "source": [
    "Mirror the exact preprocessing decisions from Step 2 so the data lines\n",
    "up with `preprocess`.  The split stays identical (random_state=42) for\n",
    "apples-to-apples comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd87a587",
   "metadata": {},
   "source": [
    "## 4C ― Random Forest with minimal tuning (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dceca8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.6666082256443399,\n",
       " 'RMSE': np.float64(3.064663596902431),\n",
       " 'R²': 0.7911703824817187}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.base import clone          # ← add this import\n",
    "\n",
    "rf_base = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Use sklearn's clone to make a *fresh* copy of the fitted pre-processor\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"prep\", clone(preprocess)),        # ✅ replace joblib.clone\n",
    "    (\"rf\",   rf_base)\n",
    "])\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "y_pred_rf = rf_pipeline.predict(X_val)\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(((y_true - y_pred) ** 2).mean())\n",
    "    r2  = r2_score(y_true, y_pred)\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"R²\": r2}\n",
    "\n",
    "scores_rf = metrics(y_val, y_pred_rf)\n",
    "scores_rf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fead614",
   "metadata": {},
   "source": [
    "## 4D ― Gradient-Boosting with coarse hyper-search (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1510cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'gb__subsample': 0.8, 'gb__n_estimators': 600, 'gb__max_depth': 2, 'gb__learning_rate': 0.1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.6705381806817967,\n",
       " 'RMSE': np.float64(2.7757099648405865),\n",
       " 'R²': 0.8286931841473344}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingRegressor(random_state=RANDOM_STATE)\n",
    "\n",
    "param_dist = {\n",
    "    \"gb__n_estimators\":  [200, 400, 600],\n",
    "    \"gb__learning_rate\": [0.03, 0.05, 0.1],\n",
    "    \"gb__max_depth\":     [2, 3, 4],\n",
    "    \"gb__subsample\":     [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "gb_pipeline = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"gb\",   gb)\n",
    "])\n",
    "\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=RANDOM_STATE)\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    gb_pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Best params:\", search.best_params_)\n",
    "best_gb = search.best_estimator_\n",
    "\n",
    "y_pred_gb = best_gb.predict(X_val)\n",
    "scores_gb = metrics(y_val, y_pred_gb)\n",
    "scores_gb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181883fa",
   "metadata": {},
   "source": [
    "**GradientBoostingRegressor** captures non-linearities via additive trees.  \n",
    "We run a *RandomizedSearchCV* (20 combos × 10-fold CV) over key knobs:\n",
    "\n",
    "| Hyper-param | Effect |\n",
    "|-------------|--------|\n",
    "| `n_estimators` / `learning_rate` | trade-off bias vs variance |\n",
    "| `max_depth` | tree complexity |\n",
    "| `subsample` | stochastic boosting for extra regularisation |\n",
    "\n",
    "The best model is evaluated on the same validation set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a908a6f",
   "metadata": {},
   "source": [
    "## 4E ― Compare all models so far (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "677b3191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  Re-establish baseline scores (Dummy + LinearRegression)     ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "\n",
    "# ----- Dummy (median) -----\n",
    "dummy_pipe = Pipeline([\n",
    "    (\"prep\", clone(preprocess)),        # fresh copy of fitted transformer\n",
    "    (\"reg\",  DummyRegressor(strategy=\"median\"))\n",
    "])\n",
    "dummy_pipe.fit(X_train, y_train)\n",
    "y_pred_dummy = dummy_pipe.predict(X_val)\n",
    "scores_dummy = metrics(y_val, y_pred_dummy)\n",
    "\n",
    "# ----- LinearRegression -----\n",
    "lin_pipe = Pipeline([\n",
    "    (\"prep\", clone(preprocess)),\n",
    "    (\"reg\",  LinearRegression())\n",
    "])\n",
    "lin_pipe.fit(X_train, y_train)\n",
    "y_pred_lin = lin_pipe.predict(X_val)\n",
    "scores_lin = metrics(y_val, y_pred_lin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6de879c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dummy</th>\n",
       "      <td>2.656</td>\n",
       "      <td>6.864</td>\n",
       "      <td>-0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearReg</th>\n",
       "      <td>2.227</td>\n",
       "      <td>9.304</td>\n",
       "      <td>-0.925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.667</td>\n",
       "      <td>3.065</td>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradBoost</th>\n",
       "      <td>0.671</td>\n",
       "      <td>2.776</td>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                MAE   RMSE     R²\n",
       "Dummy         2.656  6.864 -0.047\n",
       "LinearReg     2.227  9.304 -0.925\n",
       "RandomForest  0.667  3.065  0.791\n",
       "GradBoost     0.671  2.776  0.829"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    [scores_dummy, scores_lin, scores_rf, scores_gb],\n",
    "    index=[\"Dummy\", \"LinearReg\", \"RandomForest\", \"GradBoost\"]\n",
    ").round(3)\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedaa089",
   "metadata": {},
   "source": [
    "Put every model’s MAE / RMSE / R² side-by-side.  \n",
    "Typical pattern you should see:\n",
    "\n",
    "* **RandomForest** → big drop in both MAE & RMSE, R² positive.  \n",
    "* **GradBoost**   → often edges out RF after tuning.\n",
    "\n",
    "If either tree model *fails* to beat the Dummy baseline, double-check\n",
    "that `preprocess` is the *fitted* version and that target/leak issues\n",
    "aren’t creeping in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6f331",
   "metadata": {},
   "source": [
    "## 4F ― Save the best model & commit (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea6428e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/model_gradboost.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(best_gb, \"../data/model_gradboost.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a424b84",
   "metadata": {},
   "source": [
    "Persist the tuned GradientBoost model so future notebooks (or a web API)\n",
    "can load it quickly:\n",
    "\n",
    "```python\n",
    "model = joblib.load(\"../data/model_gradboost.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b66848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: pathspec 'notebooks/04_tree_models.ipynb' did not match any files\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   02_preprocessing.ipynb\u001b[m\n",
      "\t\u001b[31mmodified:   03_baselines.ipynb\u001b[m\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31m../data/data.csv\u001b[m\n",
      "\t\u001b[31m../data/model_gradboost.pkl\u001b[m\n",
      "\t\u001b[31m04_tree_models.ipynb\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git add notebooks/04_tree_models.ipynb data/model_gradboost.pkl\n",
    "!git commit -m \"Step 4: RandomForest + tuned GradientBoost with results table\"\n",
    "!git push\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864f257d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asteroid-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
