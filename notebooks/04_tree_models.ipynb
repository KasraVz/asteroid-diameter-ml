{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb2cf6b",
   "metadata": {},
   "source": [
    "# Thesis Documentation for `04_tree_models.ipynb`\n",
    "\n",
    "This document provides a detailed methodological justification for the steps in the 04_tree_models.ipynb notebook. The focus of this stage is to train, tune, and evaluate advanced tree-based ensemble models and compare their performance against the established baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f33a2",
   "metadata": {},
   "source": [
    "## 4-A & 4-B: Workspace Initialization and Data Recreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53046a31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T21:30:21.469321Z",
     "iopub.status.busy": "2025-07-07T21:30:21.469015Z",
     "iopub.status.idle": "2025-07-07T21:30:23.225152Z",
     "shell.execute_reply": "2025-07-07T21:30:23.224353Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4-A: Imports & data reload\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from sklearn.model_selection import RepeatedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "DATA      = Path(\"../data/asteroids_clean.csv\")\n",
    "PREPROC_P = Path(\"../data/preprocess.pkl\")\n",
    "\n",
    "df         = pd.read_csv(DATA)\n",
    "preprocess = joblib.load(PREPROC_P)\n",
    "\n",
    "# 4-B: Rebuild X/y and split once\n",
    "TARGET = \"diameter\"\n",
    "DROP_ALWAYS = [\"Unnamed: 0\", \"GM\", \"G\", \"IR\", \"extent\",\n",
    "               \"UB\", \"BV\", \"spec_B\", \"spec_T\", \"name\", \"per_y\"]\n",
    "\n",
    "X = df.drop(columns=[TARGET] + DROP_ALWAYS, errors=\"ignore\").copy()\n",
    "y = df[TARGET].copy()\n",
    "X[\"condition_code\"] = X[\"condition_code\"].astype(\"object\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804d925",
   "metadata": {},
   "source": [
    "# Thesis Justification\n",
    "## Objective:\n",
    "To prepare the workspace for training advanced models, ensuring complete consistency with the previous stages of the project.\n",
    "\n",
    "## Methodology:\n",
    "The environment is initialized by importing the necessary libraries, including the ensemble model classes (`RandomForestRegressor`, `GradientBoostingRegressor`) and hyperparameter tuning tools (`RandomizedSearchCV`). The cleaned data and the fitted `preprocess` object are loaded, and the data is partitioned into identical training and validation sets using the established `RANDOM_STATE`.\n",
    "\n",
    "## Justification:\n",
    "This rigorous setup is crucial for maintaining the scientific validity of the model comparison. By reusing the exact same fitted preprocessor and `random_state` for the data split, we isolate the change in performance to be purely a function of the model algorithm itself, eliminating data-related confounding variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fbc084",
   "metadata": {},
   "source": [
    "Load the clean data and the *fitted* `preprocess` object so we can bolt\n",
    "tree models on top.  We import RandomForest & GradientBoosting plus\n",
    "cross-validation helpers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940c7fc1",
   "metadata": {},
   "source": [
    "Mirror the exact preprocessing decisions from Step 2 so the data lines\n",
    "up with `preprocess`.  The split stays identical (random_state=42) for\n",
    "apples-to-apples comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd87a587",
   "metadata": {},
   "source": [
    "## 4C ― Random Forest Regressor with minimal tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dceca8b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T21:30:23.239171Z",
     "iopub.status.busy": "2025-07-07T21:30:23.238979Z",
     "iopub.status.idle": "2025-07-07T21:30:23.855651Z",
     "shell.execute_reply": "2025-07-07T21:30:23.855148Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.6666082256443401,\n",
       " 'RMSE': np.float64(3.064663596902431),\n",
       " 'R²': 0.7911703824817187}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.base import clone          # ← add this import\n",
    "\n",
    "rf_base = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Use sklearn's clone to make a *fresh* copy of the fitted pre-processor\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"prep\", clone(preprocess)),        # ✅ replace joblib.clone\n",
    "    (\"rf\",   rf_base)\n",
    "])\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "y_pred_rf = rf_pipeline.predict(X_val)\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(((y_true - y_pred) ** 2).mean())\n",
    "    r2  = r2_score(y_true, y_pred)\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"R²\": r2}\n",
    "\n",
    "scores_rf = metrics(y_val, y_pred_rf)\n",
    "scores_rf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c7ce3d",
   "metadata": {},
   "source": [
    "# Thesis Justification\n",
    "## Objective:\n",
    "To evaluate the performance of a Random Forest model, a powerful ensemble method capable of capturing non-linear relationships and feature interactions.\n",
    "\n",
    "## Methodology:\n",
    "A `RandomForestRegressor` is instantiated with a set of reasonable default hyperparameters. This model is then placed into a `Pipeline` with a `clone` of the fitted preprocessor. The pipeline is trained, and its performance is evaluated on the validation set.\n",
    "\n",
    "## Justification:\n",
    "\n",
    " - Model Choice: Linear models assume an additive, linear relationship between features and the target. The physics of asteroids may involve complex, non-linear interactions. Random Forest, an ensemble of decision trees, is an excellent next step as it makes no such assumptions and can model these intricate patterns effectively.\n",
    "\n",
    " - Hyperparameters: The chosen parameters (`n_estimators=300`, `min_samples_leaf=2`) represent a sensible starting point, creating a reasonably large and regularized forest without extensive tuning. `n_jobs=-1` is used to parallelize training and accelerate the process.\n",
    "\n",
    " - `clone(preprocess)`: Using `clone` is a critical best practice. It creates a fresh, unfitted copy of the preprocessing pipeline structure, which is then fitted inside the main `rf_pipeline` when `.fit()` is called. This ensures that the model can be treated as a single, self-contained object, which is essential for cross-validation and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fead614",
   "metadata": {},
   "source": [
    "## 4-D: GradientBoostingRegressor with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1510cb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T21:30:23.858025Z",
     "iopub.status.busy": "2025-07-07T21:30:23.857832Z",
     "iopub.status.idle": "2025-07-07T21:30:58.657732Z",
     "shell.execute_reply": "2025-07-07T21:30:58.657111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'gb__subsample': 0.8, 'gb__n_estimators': 600, 'gb__max_depth': 2, 'gb__learning_rate': 0.1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.6705381806817967,\n",
       " 'RMSE': np.float64(2.7757099648405865),\n",
       " 'R²': 0.8286931841473344}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingRegressor(random_state=RANDOM_STATE)\n",
    "\n",
    "param_dist = {\n",
    "    \"gb__n_estimators\":  [200, 400, 600],\n",
    "    \"gb__learning_rate\": [0.03, 0.05, 0.1],\n",
    "    \"gb__max_depth\":     [2, 3, 4],\n",
    "    \"gb__subsample\":     [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "gb_pipeline = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"gb\",   gb)\n",
    "])\n",
    "\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=RANDOM_STATE)\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    gb_pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Best params:\", search.best_params_)\n",
    "best_gb = search.best_estimator_\n",
    "\n",
    "y_pred_gb = best_gb.predict(X_val)\n",
    "scores_gb = metrics(y_val, y_pred_gb)\n",
    "scores_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b4197",
   "metadata": {},
   "source": [
    "# Thesis Justification\n",
    "## Objective:\n",
    "To train a Gradient Boosting model and systematically search for an optimal set of hyperparameters to maximize its predictive performance.\n",
    "\n",
    "## Methodology:\n",
    "A `GradientBoostingRegressor` is combined with the preprocessor in a pipeline. A `RandomizedSearchCV` is configured to explore a distribution of key hyperparameters using a robust cross-validation strategy. The best-performing model from this search is then evaluated on the hold-out validation set.\n",
    "\n",
    "## Justification:\n",
    "\n",
    " - Model Choice: Gradient Boosting is another state-of-the-art ensemble method. Unlike Random Forest, which builds trees independently, Gradient Boosting builds them sequentially, with each new tree correcting the errors of the previous one. This often leads to higher predictive accuracy, making it a logical model to test.\n",
    "\n",
    " - `RandomizedSearchCV` vs. `GridSearchCV`: For a hyperparameter space of this size (3x3x3x3 = 81 combinations), an exhaustive `GridSearchCV` would be computationally expensive. `RandomizedSearchCV` is a more efficient alternative that samples a fixed number of parameter combinations (`n_iter=20`). Research has shown that randomized search can often find models that are as good as or better than those found by grid search in a fraction of the time.\n",
    "\n",
    " - Cross-Validation (`RepeatedKFold`): Standard k-fold cross-validation can have high variance depending on how the folds are split. `RepeatedKFold` (with 5 splits and 2 repeats) mitigates this by running the k-fold process multiple times with different random shuffles. This provides a more stable and reliable estimate of a model's true performance during the search.\n",
    "\n",
    " - Scoring Metric: `neg_root_mean_squared_error` is chosen as the scoring metric for the search. It is the negative of RMSE, used because scikit-learn's search functions are designed to maximize a score. Maximizing negative RMSE is equivalent to minimizing RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181883fa",
   "metadata": {},
   "source": [
    "**GradientBoostingRegressor** captures non-linearities via additive trees.  \n",
    "We run a *RandomizedSearchCV* (20 combos × 10-fold CV) over key knobs:\n",
    "\n",
    "| Hyper-param | Effect |\n",
    "|-------------|--------|\n",
    "| `n_estimators` / `learning_rate` | trade-off bias vs variance |\n",
    "| `max_depth` | tree complexity |\n",
    "| `subsample` | stochastic boosting for extra regularisation |\n",
    "\n",
    "The best model is evaluated on the same validation set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a908a6f",
   "metadata": {},
   "source": [
    "# 4-E & 4-F: Final Comparison and Model Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec01d8e",
   "metadata": {},
   "source": [
    "### 4-E: Re-establish baseline scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "677b3191",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T21:30:58.661334Z",
     "iopub.status.busy": "2025-07-07T21:30:58.661121Z",
     "iopub.status.idle": "2025-07-07T21:30:58.694761Z",
     "shell.execute_reply": "2025-07-07T21:30:58.694347Z"
    }
   },
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  Re-establish baseline scores (Dummy + LinearRegression)     ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "\n",
    "# ----- Dummy (median) -----\n",
    "dummy_pipe = Pipeline([\n",
    "    (\"prep\", clone(preprocess)),        # fresh copy of fitted transformer\n",
    "    (\"reg\",  DummyRegressor(strategy=\"median\"))\n",
    "])\n",
    "dummy_pipe.fit(X_train, y_train)\n",
    "y_pred_dummy = dummy_pipe.predict(X_val)\n",
    "scores_dummy = metrics(y_val, y_pred_dummy)\n",
    "\n",
    "# ----- LinearRegression -----\n",
    "lin_pipe = Pipeline([\n",
    "    (\"prep\", clone(preprocess)),\n",
    "    (\"reg\",  LinearRegression())\n",
    "])\n",
    "lin_pipe.fit(X_train, y_train)\n",
    "y_pred_lin = lin_pipe.predict(X_val)\n",
    "scores_lin = metrics(y_val, y_pred_lin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebf38b8",
   "metadata": {},
   "source": [
    "### Create comparison table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de879c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T21:30:58.696971Z",
     "iopub.status.busy": "2025-07-07T21:30:58.696800Z",
     "iopub.status.idle": "2025-07-07T21:30:58.708308Z",
     "shell.execute_reply": "2025-07-07T21:30:58.707885Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dummy</th>\n",
       "      <td>2.656</td>\n",
       "      <td>6.864</td>\n",
       "      <td>-0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearReg</th>\n",
       "      <td>2.227</td>\n",
       "      <td>9.304</td>\n",
       "      <td>-0.925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.667</td>\n",
       "      <td>3.065</td>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradBoost</th>\n",
       "      <td>0.671</td>\n",
       "      <td>2.776</td>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                MAE   RMSE     R²\n",
       "Dummy         2.656  6.864 -0.047\n",
       "LinearReg     2.227  9.304 -0.925\n",
       "RandomForest  0.667  3.065  0.791\n",
       "GradBoost     0.671  2.776  0.829"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    [scores_dummy, scores_lin, scores_rf, scores_gb],\n",
    "    index=[\"Dummy\", \"LinearReg\", \"RandomForest\", \"GradBoost\"]\n",
    ").round(3)\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedaa089",
   "metadata": {},
   "source": [
    "Put every model’s MAE / RMSE / R² side-by-side.  \n",
    "Typical pattern you should see:\n",
    "\n",
    "* **RandomForest** → big drop in both MAE & RMSE, R² positive.  \n",
    "* **GradBoost**   → often edges out RF after tuning.\n",
    "\n",
    "If either tree model *fails* to beat the Dummy baseline, double-check\n",
    "that `preprocess` is the *fitted* version and that target/leak issues\n",
    "aren’t creeping in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6f331",
   "metadata": {},
   "source": [
    "## 4F ― Save the best model & commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea6428e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T21:30:58.710624Z",
     "iopub.status.busy": "2025-07-07T21:30:58.710444Z",
     "iopub.status.idle": "2025-07-07T21:30:58.727775Z",
     "shell.execute_reply": "2025-07-07T21:30:58.727292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/model_gradboost.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib, pathlib\n",
    "joblib.dump(best_gb, pathlib.Path(\"../data/model_gradboost.pkl\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38811760",
   "metadata": {},
   "source": [
    "# Thesis Justification\n",
    "## Objective:\n",
    "To create a final, comprehensive comparison of all evaluated models and to save the best-performing model for interpretation and future use.\n",
    "\n",
    "## Methodology:\n",
    "The baseline models are re-run within the notebook to ensure a fair comparison. The scores from all four models (`Dummy`, `LinearRegression`, `RandomForest`, `GradientBoosting`) are compiled into a single pandas DataFrame. The best model from the search (`best_gb`) is then serialized and saved to a file using `joblib`.\n",
    "\n",
    "## Justification:\n",
    "\n",
    " - Final Comparison Table: The results table provides the definitive evidence for model selection. The dramatic improvement in all metrics (lower MAE/RMSE, higher R²) for the tree-based models compared to the linear model strongly indicates the presence of significant non-linear relationships and feature interactions in the data, which the ensemble methods successfully captured. The table clearly shows that the tuned `GradientBoostingRegressor` is the superior model.\n",
    "\n",
    " - Model Persistence: Saving the `best_gb` object is the final step of the modeling phase. This object is not just a model; it is a complete, fitted pipeline that encapsulates all preprocessing and prediction logic. This single file can now be loaded in the final notebook for model interpretation (e.g., feature importance, SHAP analysis) and could be deployed in a production environment to make predictions on new asteroid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b66848",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add notebooks/04_tree_models.ipynb data/model_gradboost.pkl\n",
    "!git commit -m \"Step 4: RandomForest + tuned GradientBoost with results table\"\n",
    "!git push\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asteroid-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
